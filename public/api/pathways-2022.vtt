WEBVTT

1
00:00:00.000 --> 00:00:03.291
<v Eric>Welcome back to Strollcast! I'm Eric.

2
00:00:03.591 --> 00:00:08.215
<v Maya>And I'm Maya. We're your AI hosts, here to make research accessible while you're on the move.

3
00:00:08.515 --> 00:00:19.878
<v Eric>Today we're diving into one of Google's most ambitious infrastructure papers—Pathways: Asynchronous Distributed Dataflow for ML, published in 2022.

4
00:00:20.178 --> 00:00:30.862
<v Maya>This paper is fascinating because it's not about a new model architecture or training algorithm. It's about fundamentally rethinking how we orchestrate computation across thousands of accelerators.

5
00:00:31.162 --> 00:00:50.571
<v Eric>Exactly. And the author list reads like a who's who of distributed systems—Paul Barham, Jeff Dean, Sanjay Ghemawat. These are the people who built MapReduce, TensorFlow, and countless other systems that power modern computing.

6
00:00:50.871 --> 00:00:59.309
<v Maya>So what problem is Pathways trying to solve? Haven't we already figured out distributed training with systems like Megatron, DeepSpeed, and PyTorch FSDP?

7
00:00:59.609 --> 00:01:19.619
<v Eric>Great question. Those systems are excellent at what they do, but they're optimized for a specific pattern—SPMD, or Single Program Multiple Data. You write one program, and it runs the same computation across all your devices.

8
00:01:19.919 --> 00:01:29.349
<v Maya>Right, that's how most distributed training works today. You shard your model and data across GPUs, and each GPU runs the same operations on different chunks.

9
00:01:29.649 --> 00:01:49.058
<v Eric>But Google's researchers were looking ahead to a future where we might want something more flexible. What if you want different parts of your system doing completely different things? What if you want one group of TPUs running a vision model while another runs a language model, and they need to communicate?

10
00:01:49.358 --> 00:01:55.810
<v Maya>Ah, so this is about heterogeneous computation—mixing different types of workloads across a shared infrastructure.

11
00:01:56.110 --> 00:02:12.254
<v Eric>Exactly. And it's also about making it easier to experiment with new parallelism strategies. With current systems, implementing a new form of parallelism often means rewriting significant portions of your infrastructure.

12
00:02:12.554 --> 00:02:18.771
<v Maya>Let's set the stage a bit. What does the landscape of ML infrastructure look like, and where does Pathways fit in?

13
00:02:19.071 --> 00:02:30.304
<v Eric>So at the time this paper was written, there were roughly two approaches to distributed ML. The first is what I'd call the multi-controller approach.

14
00:02:30.604 --> 00:02:36.691
<v Maya>Multi-controller—meaning each accelerator or group of accelerators has its own controller making decisions?

15
00:02:36.991 --> 00:02:50.078
<v Eric>Right. Systems like PyTorch's distributed data parallel work this way. Each process independently figures out what to do, and they coordinate through collective operations like all-reduce.

16
00:02:50.378 --> 00:02:56.491
<v Maya>That's actually pretty elegant. No single point of failure, and you can scale horizontally just by adding more processes.

17
00:02:56.791 --> 00:03:14.789
<v Eric>It is elegant, and it works really well for SPMD workloads. But it has limitations. Because there's no central coordinator, it's hard to implement more complex patterns. Every process needs to know the full execution plan upfront.

18
00:03:15.089 --> 00:03:18.746
<v Maya>And I'm guessing that makes dynamic or heterogeneous workloads difficult?

19
00:03:19.046 --> 00:03:32.003
<v Eric>Exactly. If you want process A to send data to process B, but only sometimes, based on some runtime condition—that gets complicated fast in a multi-controller world.

20
00:03:32.303 --> 00:03:35.908
<v Maya>So what's the alternative? A single controller that orchestrates everything?

21
00:03:36.208 --> 00:03:51.254
<v Eric>That's the other extreme. TensorFlow 1.x actually worked somewhat like this, with a central coordinator managing the dataflow graph. But the problem is that a single controller becomes a bottleneck at scale.

22
00:03:51.554 --> 00:03:57.850
<v Maya>If every operation needs to check in with the controller, you're limited by how fast that controller can make decisions.

23
00:03:58.150 --> 00:04:08.677
<v Eric>Right. And when you're running on thousands of TPUs, that controller would need to make millions of scheduling decisions per second. It just doesn't scale.

24
00:04:08.977 --> 00:04:16.997
<v Maya>So Pathways is trying to get the best of both worlds? The flexibility of single-controller coordination with the scalability of distributed execution?

25
00:04:17.297 --> 00:04:23.331
<v Eric>That's exactly it. And the key insight is separating the control plane from the data plane.

26
00:04:23.631 --> 00:04:26.818
<v Maya>Control plane and data plane—those terms come from networking, right?

27
00:04:27.118 --> 00:04:40.101
<v Eric>Yes! In networking, the control plane decides where packets should go, while the data plane actually moves the packets. Pathways applies the same separation to ML computation.

28
00:04:40.401 --> 00:04:47.767
<v Maya>So the control plane in Pathways decides what computations should happen and where, while the data plane actually executes those computations?

29
00:04:48.067 --> 00:05:02.592
<v Eric>Exactly. And here's the clever part—the control plane can run ahead of the data plane. It can make scheduling decisions for future operations even while current operations are still executing.

30
00:05:02.892 --> 00:05:13.001
<v Maya>Oh, that's interesting. So while TPU number 47 is crunching through a matrix multiplication, the control plane is already figuring out what TPU 47 should do next?

31
00:05:13.301 --> 00:05:24.899
<v Eric>Right. And not just what it should do next, but what it should do ten steps from now. The control plane builds up a queue of work for each accelerator.

32
00:05:25.199 --> 00:05:28.621
<v Maya>That sounds like buffering or pipelining in the control logic itself.

33
00:05:28.921 --> 00:05:43.681
<v Eric>Exactly—they call it buffered dispatch. The control plane dispatches operations to accelerators well before they're needed, so the accelerators always have work queued up and ready to go.

34
00:05:43.981 --> 00:05:50.250
<v Maya>But wait, how can you schedule future operations if you don't know the results of current ones? Don't you have data dependencies?

35
00:05:50.550 --> 00:06:07.347
<v Eric>Great question! This is where futures come in. In Pathways, computations don't pass around actual data—they pass around futures, which are essentially promises of data that will exist in the future.

36
00:06:07.647 --> 00:06:12.662
<v Maya>Futures—like in concurrent programming? A placeholder for a value that will be computed eventually?

37
00:06:12.962 --> 00:06:29.863
<v Eric>Exactly. When the control plane schedules an operation, it doesn't wait for the inputs to be ready. It just says "this operation will consume the future from operation A and produce a new future that operation C will consume."

38
00:06:30.163 --> 00:06:35.492
<v Maya>So the control plane is essentially building up a graph of dependencies without waiting for actual values?

39
00:06:35.792 --> 00:06:53.791
<v Eric>Right. The control plane races ahead, building and scheduling the computation graph. Meanwhile, the data plane follows behind, actually executing operations as their input futures become ready.

40
00:06:54.091 --> 00:07:00.439
<v Maya>That's elegant. The control plane is never blocked waiting for data, so it can scale to handle massive graphs.

41
00:07:00.739 --> 00:07:14.113
<v Eric>And because the control plane has scheduled work in advance, the accelerators don't have to wait for scheduling decisions. They just pull work from their queues as soon as their current computation finishes.

42
00:07:14.413 --> 00:07:17.522
<v Maya>How far ahead can the control plane get? Is there a limit?

43
00:07:17.822 --> 00:07:28.950
<v Eric>In practice, they found that a few steps of lookahead is usually enough to hide the scheduling latency. The key is that the control plane is fast enough to stay ahead of the data plane.

44
00:07:29.250 --> 00:07:38.341
<v Maya>What happens when there's a genuine data dependency that affects control flow? Like, what if you need to check if a loss has converged before deciding whether to continue training?

45
00:07:38.641 --> 00:07:54.941
<v Eric>That's when the control plane has to wait. But those checkpoints are relatively rare in most ML workloads. The vast majority of operations have predictable dependencies that can be scheduled in advance.

46
00:07:55.241 --> 00:08:01.589
<v Maya>So for the steady-state of training—the forward pass, backward pass, optimizer step—the control plane can stay way ahead?

47
00:08:01.889 --> 00:08:06.618
<v Eric>Exactly. And that's where most of the compute time goes anyway.

48
00:08:06.918 --> 00:08:13.370
<v Maya>Let's talk more about why they chose a single-controller model. You mentioned scalability concerns—how do they address those?

49
00:08:13.670 --> 00:08:28.325
<v Eric>The key insight is that the control plane operations are very lightweight compared to the data plane operations. Scheduling a matrix multiplication takes microseconds. Actually executing it takes milliseconds.

50
00:08:28.625 --> 00:08:34.972
<v Maya>So even though there's a single controller, it's not a bottleneck because its work is so much faster than the actual computation?

51
00:08:35.272 --> 00:08:49.143
<v Eric>Right. And they do shard the controller across multiple machines, so it's not literally a single process. But logically, there's one coordinator that sees the whole computation graph.

52
00:08:49.443 --> 00:08:51.847
<v Maya>What are the benefits of having that global view?

53
00:08:52.147 --> 00:09:06.592
<v Eric>Several things. First, it's much easier to implement complex parallelism strategies. The controller can see all the resources and all the work, so it can make globally optimal decisions.

54
00:09:06.892 --> 00:09:10.602
<v Maya>Like deciding which operations to co-locate to minimize communication?

55
00:09:10.902 --> 00:09:24.721
<v Eric>Exactly. Or deciding when to prefetch data, or how to schedule pipeline stages to minimize bubbles. With a multi-controller system, each process only sees its local view.

56
00:09:25.021 --> 00:09:28.913
<v Maya>And you mentioned it's easier to experiment with new parallelism patterns?

57
00:09:29.213 --> 00:09:46.245
<v Eric>Yes! In a multi-controller system, implementing a new parallelism strategy often means changing code on every process and making sure they all coordinate correctly. In Pathways, you just change the controller's scheduling logic.

58
00:09:46.545 --> 00:09:53.232
<v Maya>That sounds like a huge win for research velocity. You can prototype new ideas without a massive engineering effort.

59
00:09:53.532 --> 00:10:06.802
<v Eric>That's exactly the motivation. Google wants to explore new architectures that might mix different parallelism strategies in novel ways. Having a flexible control plane makes that exploration tractable.

60
00:10:07.102 --> 00:10:10.106
<v Maya>The paper mentions gang scheduling. What's that about?

61
00:10:10.406 --> 00:10:25.348
<v Eric>Gang scheduling is about launching parallel operations together, as a group. When you have an SPMD computation that needs to run across 2048 TPUs, you want all of them to start at roughly the same time.

62
00:10:25.648 --> 00:10:28.417
<v Maya>Why does that matter? Can't they just start whenever they're ready?

63
00:10:28.717 --> 00:10:42.876
<v Eric>The problem is collective operations. When you do an all-reduce, every participant needs to reach that point before anyone can proceed. If one TPU starts late, everyone else has to wait.

64
00:10:43.176 --> 00:10:46.781
<v Maya>Oh right, the slowest member determines the speed of the whole collective.

65
00:10:47.081 --> 00:10:56.354
<v Eric>Exactly. So you want to launch the whole gang together, minimizing the skew between when different TPUs start the same operation.

66
00:10:56.654 --> 00:10:58.274
<v Maya>How does Pathways achieve that?

67
00:10:58.574 --> 00:11:13.464
<v Eric>The controller tracks the state of all accelerators and waits until the entire gang is ready before dispatching the next operation. But because of the buffered dispatch we discussed, it can queue up multiple operations in advance.

68
00:11:13.764 --> 00:11:17.186
<v Maya>So the gang scheduling happens at dispatch time, not execution time?

69
00:11:17.486 --> 00:11:34.335
<v Eric>Right. The controller waits for all TPUs to be ready to receive the next batch of work, then dispatches to all of them together. The actual execution then proceeds in lockstep because everyone received the same work at the same time.

70
00:11:34.635 --> 00:11:39.258
<v Maya>What about when you have heterogeneous computations? Different TPUs doing different things?

71
00:11:39.558 --> 00:11:55.807
<v Eric>That's where Pathways really shines. The controller can manage multiple gangs doing different work, and coordinate data transfers between them. It's like conducting an orchestra where different sections are playing different parts.

72
00:11:56.107 --> 00:12:01.253
<v Maya>Beautiful analogy. The conductor—the control plane—ensures everyone comes in at the right time.

73
00:12:01.553 --> 00:12:10.984
<v Eric>Let's talk about the actual hardware Pathways is designed for. Google's TPU infrastructure is organized into pods and islands.

74
00:12:11.284 --> 00:12:16.377
<v Maya>I've heard about TPU pods. Those are the collections of TPUs connected by high-speed interconnects, right?

75
00:12:16.677 --> 00:12:33.944
<v Eric>Right. A TPU v4 pod can have up to 4096 chips, all connected by a high-bandwidth mesh network. Within a pod, communication is very fast—hundreds of gigabytes per second between neighboring chips.

76
00:12:34.244 --> 00:12:38.372
<v Maya>That's incredible bandwidth. I assume that's important for all-reduce operations?

77
00:12:38.672 --> 00:12:53.379
<v Eric>Crucial. When you're doing data parallel training, you need to sum gradients across all devices. Fast interconnects make that feasible even at massive scale.

78
00:12:53.679 --> 00:12:55.716
<v Maya>You mentioned islands—what are those?

79
00:12:56.016 --> 00:13:10.958
<v Eric>Islands are groups of TPUs that might be in different pods or even different data centers. The interconnect between islands is slower—it's data center networking rather than the dedicated TPU mesh.

80
00:13:11.258 --> 00:13:16.535
<v Maya>So Pathways needs to handle both fast intra-pod communication and slower inter-island communication?

81
00:13:16.835 --> 00:13:28.355
<v Eric>Exactly. And the controller needs to be smart about placing computations. You want operations that communicate frequently to be in the same pod if possible.

82
00:13:28.655 --> 00:13:34.742
<v Maya>That sounds like a scheduling optimization problem—placing computations to minimize communication costs.

83
00:13:35.042 --> 00:13:45.203
<v Eric>It is, and it's a hard one. The paper discusses how Pathways handles this, but there's definitely room for more sophisticated placement strategies.

84
00:13:45.503 --> 00:13:50.519
<v Maya>How does Pathways interact with the resource manager? Who decides which TPUs a job gets?

85
00:13:50.819 --> 00:14:04.324
<v Eric>Pathways interfaces with Google's cluster management system. When you launch a Pathways job, you request a certain number of TPU cores, and the resource manager allocates them from available pools.

86
00:14:04.624 --> 00:14:07.811
<v Maya>Can a single Pathways job span multiple pods or islands?

87
00:14:08.111 --> 00:14:21.433
<v Eric>Yes! That's actually one of the key capabilities. A single logical computation can span thousands of TPUs across multiple islands, with Pathways coordinating all the data movement.

88
00:14:21.733 --> 00:14:25.260
<v Maya>How do users actually program Pathways? What does the interface look like?

89
00:14:25.560 --> 00:14:39.483
<v Eric>Pathways is designed to work with JAX, which is Google's NumPy-like library for accelerated computing. Users write JAX code, and it gets compiled and executed on Pathways.

90
00:14:39.783 --> 00:14:44.694
<v Maya>So JAX is the frontend, and Pathways is the backend that actually runs the computation?

91
00:14:44.994 --> 00:15:00.485
<v Eric>Exactly. JAX code gets compiled to XLA, which is a compiler for linear algebra operations. Then Pathways takes the XLA computation and figures out how to distribute it across the available TPUs.

92
00:15:00.785 --> 00:15:06.662
<v Maya>Does the user have to specify how to parallelize their code, or does Pathways figure it out automatically?

93
00:15:06.962 --> 00:15:22.740
<v Eric>It's a mix. For standard SPMD workloads, JAX has primitives like pmap and pjit that let users specify how arrays should be sharded across devices. Pathways respects those specifications.

94
00:15:23.040 --> 00:15:25.627
<v Maya>What about more complex parallelism patterns?

95
00:15:25.927 --> 00:15:42.776
<v Eric>That's where things get interesting. Pathways can handle arbitrary dataflow graphs, so users can express pipeline parallelism, expert parallelism, or even custom patterns that don't fit into standard categories.

96
00:15:43.076 --> 00:15:48.953
<v Maya>Expert parallelism—is that like mixture of experts models where different experts are on different devices?

97
00:15:49.253 --> 00:16:10.386
<v Eric>Exactly! In a mixture of experts model, you might have hundreds of expert sub-networks, each on different TPUs. Tokens get routed to different experts based on a gating function. Pathways can handle that routing and the heterogeneous computation it implies.

98
00:16:10.686 --> 00:16:15.101
<v Maya>That's a great example of why you'd want something more flexible than pure SPMD.

99
00:16:15.401 --> 00:16:23.760
<v Eric>Let's talk about how Pathways handles pipeline parallelism specifically. This is one of the benchmarks they show in the paper.

100
00:16:24.060 --> 00:16:30.408
<v Maya>Pipeline parallelism is where you split the model into stages and pass activations between them, right? Like an assembly line?

101
00:16:30.708 --> 00:16:43.508
<v Eric>Exactly. Stage one processes microbatch one, then passes it to stage two while starting on microbatch two. Everyone's working in parallel on different microbatches.

102
00:16:43.808 --> 00:16:52.167
<v Maya>We talked about this a lot in the Megatron episode. The challenge is minimizing pipeline bubbles—those idle times when stages are waiting for work.

103
00:16:52.467 --> 00:17:05.267
<v Eric>Right. And Pathways can implement the same interleaved schedules that Megatron uses. But because of the single-controller model, it's easier to experiment with different schedules.

104
00:17:05.567 --> 00:17:11.418
<v Maya>How does Pathways express pipeline parallelism? Is it built into the system, or does the user specify it?

105
00:17:11.718 --> 00:17:25.877
<v Eric>The user specifies which parts of the model go on which devices, and Pathways handles the coordination. The key is that Pathways sees the whole graph, so it can schedule the pipeline stages efficiently.

106
00:17:26.177 --> 00:17:29.129
<v Maya>And the asynchronous dispatch helps here too, I assume?

107
00:17:29.429 --> 00:17:43.770
<v Eric>Absolutely. The control plane can schedule several microbatches' worth of operations in advance. Each stage knows exactly what work is coming and can execute without waiting for real-time scheduling decisions.

108
00:17:44.070 --> 00:17:47.231
<v Maya>What performance do they report for pipeline parallelism?

109
00:17:47.531 --> 00:18:03.230
<v Eric>They show that Pathways achieves comparable throughput to dedicated SPMD implementations, even for 16-stage pipelines. That's impressive because pipeline parallelism adds coordination overhead.

110
00:18:03.530 --> 00:18:06.822
<v Maya>So the asynchronous control plane successfully hides that overhead?

111
00:18:07.122 --> 00:18:14.358
<v Eric>Exactly. The accelerators see a steady stream of work, just like they would in an SPMD setup.

112
00:18:14.658 --> 00:18:18.837
<v Maya>You mentioned that Pathways can span multiple islands. How does that work in practice?

113
00:18:19.137 --> 00:18:29.586
<v Eric>This is one of the more challenging scenarios. When your computation spans multiple data centers, the communication latency jumps from microseconds to milliseconds.

114
00:18:29.886 --> 00:18:33.413
<v Maya>That's three orders of magnitude slower! How do you hide that latency?

115
00:18:33.713 --> 00:18:42.908
<v Eric>The key is computation-communication overlap. While one island is waiting for data from another, it can be working on other computations.

116
00:18:43.208 --> 00:18:47.701
<v Maya>But doesn't that require careful scheduling to have useful work available during those waits?

117
00:18:48.001 --> 00:18:58.267
<v Eric>It does, and that's where the single-controller model helps. The controller can see the whole graph and schedule operations to maximize overlap.

118
00:18:58.567 --> 00:19:01.623
<v Maya>What kinds of workloads benefit from multi-island training?

119
00:19:01.923 --> 00:19:17.936
<v Eric>Any workload that's too big for a single pod, or workloads where you want to use specialized hardware in different locations. For example, you might have vision TPUs in one island and language model TPUs in another.

120
00:19:18.236 --> 00:19:21.894
<v Maya>And Pathways coordinates the whole thing as a single logical computation?

121
00:19:22.194 --> 00:19:32.172
<v Eric>Exactly. From the user's perspective, it's one program running on one system. Pathways handles all the distributed complexity.

122
00:19:32.472 --> 00:19:36.077
<v Maya>Let's dig into the performance numbers. What benchmarks do they report?

123
00:19:36.377 --> 00:19:51.006
<v Eric>The headline result is that Pathways achieves essentially 100% accelerator utilization for SPMD workloads on up to 2048 TPUs. That matches dedicated SPMD implementations.

124
00:19:51.306 --> 00:19:56.974
<v Maya>Wait, I would have expected some overhead from the single-controller coordination. How do they achieve full utilization?

125
00:19:57.274 --> 00:20:10.414
<v Eric>The buffered dispatch is key. Because operations are queued in advance, the accelerators never stall waiting for scheduling. The control plane overhead is completely hidden.

126
00:20:10.714 --> 00:20:14.058
<v Maya>What about the more complex patterns like pipeline parallelism?

127
00:20:14.358 --> 00:20:28.229
<v Eric>For a 16-stage pipeline, they achieve throughput within a few percent of the theoretical optimum. The asynchronous dataflow successfully hides the coordination overhead.

128
00:20:28.529 --> 00:20:29.965
<v Maya>And multi-island training?

129
00:20:30.265 --> 00:20:44.659
<v Eric>They show that Pathways can efficiently utilize TPUs across two islands connected by data center networking. The throughput is lower than single-island, of course, but it's still practical for workloads that need that scale.

130
00:20:44.959 --> 00:20:49.583
<v Maya>What's the latency like for the control plane? How fast can it schedule operations?

131
00:20:49.883 --> 00:20:59.261
<v Eric>The paper reports that the control plane can dispatch millions of operations per second. That's fast enough to stay ahead of even the fastest accelerator computations.

132
00:20:59.561 --> 00:21:04.158
<v Maya>Impressive. What about fault tolerance? With thousands of TPUs, failures must be common.

133
00:21:04.458 --> 00:21:16.422
<v Eric>The paper doesn't go deep into fault tolerance, but the architecture supports checkpointing and recovery. The single-controller model actually helps here because there's a consistent view of the computation state.

134
00:21:16.722 --> 00:21:21.868
<v Maya>How does Pathways compare to systems like Megatron and DeepSpeed that we've covered in previous episodes?

135
00:21:22.168 --> 00:21:35.256
<v Eric>They're solving overlapping but different problems. Megatron and DeepSpeed are optimized frameworks for training large language models. They have specific parallelism strategies baked in.

136
00:21:35.556 --> 00:21:38.743
<v Maya>Like tensor parallelism and ZeRO-style data parallelism?

137
00:21:39.043 --> 00:21:50.145
<v Eric>Exactly. Those systems are incredibly well-optimized for their target workloads. If you're training a standard Transformer at scale, they're probably your best bet.

138
00:21:50.445 --> 00:21:52.613
<v Maya>Where does Pathways offer advantages?

139
00:21:52.913 --> 00:22:04.799
<v Eric>Flexibility. If you want to try a new parallelism strategy, or train a model architecture that doesn't fit the standard patterns, Pathways gives you a foundation to build on.

140
00:22:05.099 --> 00:22:07.319
<v Maya>It's more like infrastructure than a framework?

141
00:22:07.619 --> 00:22:22.927
<v Eric>That's a good way to put it. Pathways is the orchestration layer that frameworks could build on top of. It handles the hard problems of distributed scheduling so researchers can focus on ML innovation.

142
00:22:23.227 --> 00:22:25.630
<v Maya>What about compared to TensorFlow or PyTorch?

143
00:22:25.930 --> 00:22:40.585
<v Eric>Those are more like end-to-end ML platforms. They include everything from automatic differentiation to model serving. Pathways is narrower—it's specifically about distributed execution.

144
00:22:40.885 --> 00:22:43.340
<v Maya>Does Pathways replace TensorFlow at Google?

145
00:22:43.640 --> 00:22:57.146
<v Eric>Not exactly. JAX, which runs on Pathways, has become the preferred framework for many Google ML teams. But TensorFlow is still widely used, especially in production serving.

146
00:22:57.446 --> 00:23:02.278
<v Maya>The paper talks about enabling "new systems and ML research ideas." What do they have in mind?

147
00:23:02.578 --> 00:23:13.576
<v Eric>One vision is what they call sparsely activated models. Instead of running the whole model on every input, you dynamically select which parts to activate.

148
00:23:13.876 --> 00:23:16.932
<v Maya>Like mixture of experts, but maybe even more extreme?

149
00:23:17.232 --> 00:23:33.193
<v Eric>Exactly. Imagine a model with thousands of specialized components, and each input only activates a small fraction of them. That requires dynamic, heterogeneous computation that's hard to express in SPMD frameworks.

150
00:23:33.493 --> 00:23:36.314
<v Maya>And Pathways' flexible scheduling would help with that?

151
00:23:36.614 --> 00:23:45.627
<v Eric>Right. The control plane can handle the dynamic routing and ensure that the right experts are activated for each input.

152
00:23:45.927 --> 00:23:51.438
<v Maya>What about multi-modal models? Those seem like another case where you'd want heterogeneous computation.

153
00:23:51.738 --> 00:24:05.531
<v Eric>Definitely. A model that processes text, images, and audio might want different architectures for each modality. Pathways can coordinate computation across specialized subnetworks.

154
00:24:05.831 --> 00:24:13.067
<v Maya>This seems related to the Google Brain paper on Pathways the system is named after—the vision of a single model that can do many tasks?

155
00:24:13.367 --> 00:24:28.231
<v Eric>Exactly. Jeff Dean and others have written about moving toward more general AI systems that can handle diverse tasks. That requires infrastructure that can support diverse computation patterns.

156
00:24:28.531 --> 00:24:32.554
<v Maya>Every system has trade-offs. What are the downsides of the Pathways approach?

157
00:24:32.854 --> 00:24:43.015
<v Eric>The single-controller model, while flexible, does add complexity. You need careful engineering to ensure the control plane doesn't become a bottleneck.

158
00:24:43.315 --> 00:24:48.879
<v Maya>And there's probably higher latency for starting new computations compared to multi-controller systems?

159
00:24:49.179 --> 00:25:03.468
<v Eric>Yes, the initial setup time is higher because the controller needs to analyze the graph and plan the execution. For long-running training jobs, that's amortized away. For short tasks, it might matter.

160
00:25:03.768 --> 00:25:08.026
<v Maya>What about portability? Pathways seems very tied to TPU infrastructure.

161
00:25:08.326 --> 00:25:21.649
<v Eric>That's true. Many of the design decisions are optimized for TPU characteristics—the mesh interconnect, the synchronous execution model, the tight integration with XLA.

162
00:25:21.949 --> 00:25:24.692
<v Maya>Could similar ideas be applied to GPU clusters?

163
00:25:24.992 --> 00:25:41.841
<v Eric>In principle, yes. But GPUs have different characteristics—more heterogeneous workloads, different memory hierarchies, different interconnect topologies. You'd need to adapt the design.

164
00:25:42.141 --> 00:25:45.484
<v Maya>Are there open-source implementations of Pathways-like systems?

165
00:25:45.784 --> 00:26:00.778
<v Eric>Not directly, but JAX itself is open source, and projects like Alpa are exploring similar ideas for GPU clusters. The concepts in Pathways are influencing the broader ecosystem.

166
00:26:01.078 --> 00:26:04.004
<v Maya>What should ML practitioners take away from this paper?

167
00:26:04.304 --> 00:26:17.339
<v Eric>First, the importance of separating control plane from data plane. Even if you're not building a system like Pathways, thinking about scheduling and execution separately can clarify your design.

168
00:26:17.639 --> 00:26:20.748
<v Maya>And the buffered dispatch idea—staying ahead of the computation?

169
00:26:21.048 --> 00:26:35.441
<v Eric>Right. Any time you have latency you can predict, queuing work in advance is a powerful technique. It applies to prefetching, pipelining, all sorts of optimization.

170
00:26:35.741 --> 00:26:39.869
<v Maya>What about the choice between single-controller and multi-controller architectures?

171
00:26:40.169 --> 00:26:55.450
<v Eric>That depends on your workload. If you're doing standard SPMD, multi-controller is simpler and scales well. If you need flexibility for heterogeneous workloads, a single controller gives you more options.

172
00:26:55.750 --> 00:26:58.676
<v Maya>Any advice for people designing distributed training systems?

173
00:26:58.976 --> 00:27:10.026
<v Eric>Profile your control plane overhead. It's easy to focus on computation time and ignore scheduling latency, but at scale, that can become the bottleneck.

174
00:27:10.326 --> 00:27:14.950
<v Maya>And instrument your collective operations. Those are often where scaling breaks down.

175
00:27:15.250 --> 00:27:21.127
<v Eric>Exactly. Understanding where time goes is the first step to optimization.

176
00:27:21.427 --> 00:27:24.588
<v Maya>As we wrap up, what's the lasting impact of this work?

177
00:27:24.888 --> 00:27:35.703
<v Eric>Pathways represents a shift in how we think about ML infrastructure. It's not just about making existing workloads faster—it's about enabling new kinds of computation.

178
00:27:36.003 --> 00:27:42.768
<v Maya>Moving from "how do we train Transformers faster" to "how do we support whatever architectures researchers dream up."

179
00:27:43.068 --> 00:27:53.465
<v Eric>Right. As models get more complex and diverse, we need infrastructure that can keep up. Pathways is a step in that direction.

180
00:27:53.765 --> 00:27:55.803
<v Maya>And the ideas are spreading beyond Google?

181
00:27:56.103 --> 00:28:10.130
<v Eric>Yes. The JAX ecosystem, which builds on similar concepts, is growing rapidly. And other frameworks are adopting ideas like asynchronous dispatch and flexible parallelism.

182
00:28:10.430 --> 00:28:12.546
<v Maya>Any predictions for where this goes next?

183
00:28:12.846 --> 00:28:30.244
<v Eric>I think we'll see more systems that blur the line between training and inference. Models that learn and adapt in real-time, using dynamic computation patterns. Pathways-like infrastructure is a prerequisite for that.

184
00:28:30.544 --> 00:28:33.365
<v Maya>An exciting future! Thanks for this deep dive, Eric.

185
00:28:33.665 --> 00:28:42.390
<v Eric>My pleasure. This paper is a great example of how systems research enables ML research. Neither can advance without the other.

186
00:28:42.690 --> 00:28:54.889
<v Maya>That's all for today's episode of Strollcast. We covered Pathways, Google's asynchronous distributed dataflow system for ML—from the control plane architecture to gang scheduling to the vision of flexible, heterogeneous computation.

187
00:28:55.189 --> 00:29:07.258
<v Eric>If you enjoyed this episode, check out our previous ones on Megatron-LM, FSDP, and ZeRO. Together, they paint a picture of how the industry is tackling large-scale training.

188
00:29:07.558 --> 00:29:09.308
<v Maya>Until next time, keep strolling.

189
00:29:09.608 --> 00:29:12.299
<v Eric>And may your futures always resolve!
