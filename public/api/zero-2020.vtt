WEBVTT

1
00:00:00.800 --> 00:00:08.767
<v Eric>Welcome back to Strollcast, the podcast where we break down the papers that shaped modern machine learning. I'm Eric.

2
00:00:09.067 --> 00:00:22.285
<v Maya>And I'm Maya. We're your AI hosts, here to make research accessible while you're on the move. Today we're tackling one of the most influential systems papers in deep learning—one that literally made training models with hundreds of billions of parameters possible.

3
00:00:22.585 --> 00:00:41.707
<v Eric>That's right. We're talking about ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, published by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He from Microsoft Research.

4
00:00:42.007 --> 00:00:55.199
<v Maya>This paper came out in late 2019, and Eric, I remember exactly where I was when I first read it. I was trying to train a relatively modest transformer model, maybe 500 million parameters, and I kept running out of memory. I was so frustrated.

5
00:00:55.499 --> 00:01:01.246
<v Eric>And that frustration was universal. Everyone in the field was hitting the same wall.

6
00:01:01.546 --> 00:01:14.163
<v Maya>Exactly. When this paper dropped, it felt like someone had finally given us the keys to a door we'd been banging our heads against. Before ZeRO, if you wanted to train a really big model, you basically had two options, and neither was great.

7
00:01:14.463 --> 00:01:35.204
<v Eric>Let's set the stage for our listeners who might not remember 2019 in the AI world. GPT-2 had just come out with 1.5 billion parameters, and it was causing a huge stir. OpenAI initially didn't even release the full model because they were worried about misuse.

8
00:01:35.504 --> 00:01:47.756
<v Maya>Right, and at the time, 1.5 billion parameters seemed enormous. People were genuinely asking: is this the limit? Can we even train anything bigger? The hardware just didn't seem to support it.

9
00:01:48.056 --> 00:02:06.759
<v Eric>And here's the thing that I think surprises a lot of people even today: the model parameters themselves aren't even the main memory hog during training. There's this whole hidden iceberg of memory consumption that most people don't think about.

10
00:02:07.059 --> 00:02:13.041
<v Maya>That's the perfect setup for our first segment. Let's dig into why training large models is so memory-intensive.

11
00:02:14.141 --> 00:02:24.120
<v Eric>Okay, so let's break this down from first principles. When you're training a neural network, what's actually living in GPU memory?

12
00:02:24.420 --> 00:02:35.653
<v Maya>So most people think it's just the weights, right? The model parameters. And sure, that's part of it. For a 1.5 billion parameter model using 16-bit floating point, that's about 3 gigabytes. Sounds manageable.

13
00:02:35.953 --> 00:02:38.278
<v Eric>But that's just the tip of the iceberg.

14
00:02:38.578 --> 00:02:54.695
<v Maya>Exactly. Let me walk through everything. First, you have the parameters in 16-bit precision for the forward and backward pass. That's your 3 gigs. But then you also need to store the gradients—the derivatives you compute during backpropagation. That's another copy of every parameter, so another 3 gigs.

15
00:02:54.995 --> 00:02:57.085
<v Eric>We're already at 6 gigabytes.

16
00:02:57.385 --> 00:03:07.782
<v Maya>And we haven't even gotten to the big one yet. If you're using the Adam optimizer, which is basically the default for training transformers, you need to store what's called the optimizer states.

17
00:03:08.082 --> 00:03:12.497
<v Eric>Can you explain what those are for listeners who might not be familiar?

18
00:03:12.797 --> 00:03:30.586
<v Maya>Sure. Adam is an adaptive learning rate optimizer. It keeps track of two running averages for every single parameter in your model. The first is the mean of the gradients—that's the first moment, or momentum. The second is the mean of the squared gradients—that's the second moment, used to scale the learning rate.

19
00:03:30.886 --> 00:03:34.073
<v Eric>So that's two additional values per parameter.

20
00:03:34.373 --> 00:03:48.819
<v Maya>Right, but here's the kicker. These optimizer states need to be stored in 32-bit precision, not 16-bit. Even if you're doing mixed precision training with 16-bit activations and gradients, the optimizer states have to be 32-bit for numerical stability.

21
00:03:49.119 --> 00:03:50.216
<v Eric>Why is that?

22
00:03:50.516 --> 00:04:05.928
<v Maya>It comes down to the accumulation of small updates. Over thousands of training steps, you're adding tiny gradient values to these running averages. In 16-bit precision, you lose too much information in those small updates. The training becomes unstable or just stops converging.

23
00:04:06.228 --> 00:04:11.662
<v Eric>So we have 4 bytes per parameter for momentum, 4 bytes for variance...

24
00:04:11.962 --> 00:04:24.344
<v Maya>And you also need a 32-bit master copy of the parameters themselves for the optimizer to update. So that's another 4 bytes per parameter. In total, the optimizer states require 12 bytes per parameter.

25
00:04:24.644 --> 00:04:45.071
<v Eric>Let me add this up. For a 1.5 billion parameter model: 3 gigs for fp16 parameters, 3 gigs for fp16 gradients, and 12 bytes times 1.5 billion is... 18 gigabytes just for optimizer states.

26
00:04:45.371 --> 00:04:51.824
<v Maya>So we're at 24 gigabytes total for what the paper calls "model states." And we haven't even talked about activations yet.

27
00:04:52.124 --> 00:04:58.393
<v Eric>Ah yes, the activations. This is what catches a lot of people off guard.

28
00:04:58.693 --> 00:05:08.750
<v Maya>During the forward pass, you compute intermediate values at every layer—the activations. And you have to keep them around because you need them for the backward pass to compute gradients.

29
00:05:09.050 --> 00:05:11.506
<v Eric>How much memory do activations use?

30
00:05:11.806 --> 00:05:26.382
<v Maya>It depends heavily on your batch size and sequence length. For transformers, activation memory scales quadratically with sequence length because of the attention mechanism. For a reasonable batch size and sequence length, you could easily need another 10, 20, even 50 gigabytes.

31
00:05:26.682 --> 00:05:34.519
<v Eric>So we're looking at potentially 50, 60, 70 gigabytes for a model that only has 3 gigabytes of actual weights.

32
00:05:34.819 --> 00:05:48.742
<v Maya>Exactly! And this is why people were hitting walls. Even the best GPUs at the time, the NVIDIA V100, only had 32 gigabytes of memory. You literally couldn't fit the training state for a moderately sized model on a single GPU.

33
00:05:49.042 --> 00:06:05.473
<v Eric>The paper has this great analysis where they categorize memory into model states versus residual states—the activations, temporary buffers, and fragmentation. And they show that for GPT-2 scale models, it's actually the model states that dominate.

34
00:06:05.773 --> 00:06:10.188
<v Maya>Which is a crucial insight because it tells you where to focus your optimization efforts.

35
00:06:11.288 --> 00:06:16.382
<v Eric>So before ZeRO came along, how were people dealing with this memory problem?

36
00:06:16.682 --> 00:06:25.720
<v Maya>There were two main paradigms: data parallelism and model parallelism. Both had been around for years, but neither really solved the fundamental problem.

37
00:06:26.020 --> 00:06:30.330
<v Eric>Let's start with data parallelism since it's the more common one.

38
00:06:30.630 --> 00:06:40.740
<v Maya>Sure. Data parallelism is conceptually the simplest form of distributed training. You take your model, make complete copies of it on every GPU, and then split your training data across them.

39
00:06:41.040 --> 00:06:44.801
<v Eric>So if you have 8 GPUs and a batch size of 64...

40
00:06:45.101 --> 00:06:56.830
<v Maya>Each GPU processes 8 samples. They all do a forward pass, compute gradients, and then you average the gradients across all GPUs. Everyone updates their weights the same way, so they stay synchronized.

41
00:06:57.130 --> 00:07:00.552
<v Eric>The averaging step is the key communication operation, right?

42
00:07:00.852 --> 00:07:15.141
<v Maya>Exactly. It's called an all-reduce. Every GPU sends its gradients, they get summed up and averaged, and every GPU gets back the averaged result. Libraries like PyTorch's DistributedDataParallel and NVIDIA's NCCL make this very efficient.

43
00:07:15.441 --> 00:07:18.080
<v Eric>So what's the problem with data parallelism?

44
00:07:18.380 --> 00:07:31.232
<v Maya>The fatal flaw is that every GPU has a complete copy of everything—parameters, gradients, optimizer states. If your model needs 24 gigabytes, it needs 24 gigabytes on every single GPU.

45
00:07:31.532 --> 00:07:34.301
<v Eric>So you're not solving the memory problem at all.

46
00:07:34.601 --> 00:07:52.652
<v Maya>Not at all. You're just training faster by processing more data in parallel. If you have 64 GPUs, you're storing 64 complete copies of a 24 gigabyte model. That's over 1.5 terabytes of memory holding identical information! The paper calls this "redundant memory consumption."

47
00:07:52.952 --> 00:07:54.519
<v Eric>That's such a waste.

48
00:07:54.819 --> 00:08:03.257
<v Maya>It really is. And it means data parallelism can't help you train bigger models. It only helps you train faster on models that already fit in memory.

49
00:08:03.557 --> 00:08:05.542
<v Eric>What about model parallelism?

50
00:08:05.842 --> 00:08:16.421
<v Maya>Model parallelism takes the opposite approach. Instead of replicating the model, you partition it across GPUs. There are two main flavors: pipeline parallelism and tensor parallelism.

51
00:08:16.721 --> 00:08:18.237
<v Eric>Walk us through those.

52
00:08:18.537 --> 00:08:35.803
<v Maya>Pipeline parallelism is like an assembly line. You split the model by layers. GPU 1 has layers 1 through 10, GPU 2 has layers 11 through 20, and so on. Data flows through the pipeline—GPU 1 computes its part, sends activations to GPU 2, which computes its part, and so on.

53
00:08:36.103 --> 00:08:37.592
<v Eric>That sounds reasonable.

54
00:08:37.892 --> 00:08:52.939
<v Maya>It is, but there's a big problem called pipeline bubbles. When GPU 1 is working on the first microbatch, GPUs 2 through 8 are sitting idle. When GPU 8 is finishing up, GPUs 1 through 7 are idle. You end up with a lot of wasted compute.

55
00:08:53.239 --> 00:08:56.609
<v Eric>There are techniques to reduce the bubbles though, right?

56
00:08:56.909 --> 00:09:09.030
<v Maya>Yes, like microbatching and interleaved schedules. GPipe and PipeDream introduced clever scheduling to overlap work. But you can never fully eliminate the bubbles, and the implementation complexity is significant.

57
00:09:09.330 --> 00:09:11.028
<v Eric>And tensor parallelism?

58
00:09:11.328 --> 00:09:25.068
<v Maya>Tensor parallelism splits individual operations across GPUs. The Megatron-LM paper from NVIDIA showed how to do this for transformers. You split the big matrix multiplications in the attention and feed-forward layers across GPUs.

59
00:09:25.368 --> 00:09:28.790
<v Eric>That sounds like it would require a lot of communication.

60
00:09:29.090 --> 00:09:44.633
<v Maya>It does. After every split operation, you need to synchronize results. This means tensor parallelism really only works efficiently within a node, where you have fast NVLink connections between GPUs. Once you go across nodes, the communication overhead kills your throughput.

61
00:09:44.933 --> 00:09:50.079
<v Eric>So tensor parallelism might let you split across 8 GPUs in a node...

62
00:09:50.379 --> 00:09:54.141
<v Maya>But not across 64 GPUs spanning 8 nodes. You hit a scaling wall.

63
00:09:54.441 --> 00:09:58.882
<v Eric>And both forms of model parallelism require significant code changes, right?

64
00:09:59.182 --> 00:10:13.941
<v Maya>That's maybe the biggest practical problem. With data parallelism, you can take a standard PyTorch model and add a couple lines of code. With model parallelism, you often have to completely restructure how your model is defined and how the forward pass works. It's not plug-and-play.

65
00:10:14.241 --> 00:10:17.193
<v Eric>Which brings us to ZeRO and its key insight.

66
00:10:18.293 --> 00:10:30.858
<v Maya>Okay, so here's the fundamental question the ZeRO authors asked: In data parallelism, we have all this redundant memory. What if we could eliminate that redundancy while keeping data parallelism's simplicity and scalability?

67
00:10:31.158 --> 00:10:34.031
<v Eric>That's a really elegant reframing of the problem.

68
00:10:34.331 --> 00:10:51.049
<v Maya>Right? Instead of saying "data parallelism doesn't help with memory," they said "what if we could make data parallelism memory-efficient?" The key observation is that even though every GPU computes gradients for all parameters, each GPU only needs to update a subset of them.

69
00:10:51.349 --> 00:10:52.838
<v Eric>Explain that more.

70
00:10:53.138 --> 00:11:06.957
<v Maya>Think about it this way. In standard data parallelism, after the backward pass, every GPU has gradients for all parameters. They do an all-reduce to average them, and then every GPU independently updates all parameters using those gradients.

71
00:11:07.257 --> 00:11:12.038
<v Eric>Right, they all do the same update, which is why they stay synchronized.

72
00:11:12.338 --> 00:11:26.078
<v Maya>Exactly. But here's the thing—since they all do the exact same update, why does every GPU need to do it? What if GPU 0 only updated parameters 0 through 1000, GPU 1 updated parameters 1001 through 2000, and so on?

73
00:11:26.378 --> 00:11:29.434
<v Eric>And then you'd need to share the updated parameters...

74
00:11:29.734 --> 00:11:40.836
<v Maya>But you could do that! After each GPU updates its portion, you do an all-gather so everyone gets the full set of updated parameters. The total amount of communication is actually the same as an all-reduce.

75
00:11:41.136 --> 00:11:44.663
<v Eric>Wait, really? The same communication cost?

76
00:11:44.963 --> 00:12:02.230
<v Maya>Yes! An all-reduce is mathematically equivalent to a reduce-scatter followed by an all-gather. In standard data parallelism, you're doing an all-reduce on gradients. In ZeRO, you're doing a reduce-scatter on gradients, then each GPU updates its portion, then an all-gather on parameters. Same total bytes moved.

77
00:12:02.530 --> 00:12:07.310
<v Eric>But now each GPU only stores a fraction of the optimizer states.

78
00:12:07.610 --> 00:12:23.859
<v Maya>Exactly! That's ZeRO Stage 1—partition the optimizer states. If you have 64 GPUs, each one stores 1/64th of the optimizer states. For our earlier example, that takes you from 18 gigabytes of optimizer states per GPU down to less than 300 megabytes.

79
00:12:24.159 --> 00:12:26.014
<v Eric>That's a huge savings.

80
00:12:26.314 --> 00:12:36.240
<v Maya>The name ZeRO stands for Zero Redundancy Optimizer, and the name really captures what's happening. You're eliminating the redundant storage of optimizer states across GPUs.

81
00:12:36.540 --> 00:12:38.996
<v Eric>But they didn't stop at optimizer states.

82
00:12:39.296 --> 00:12:52.932
<v Maya>No, they went further. ZeRO Stage 2 partitions the gradients as well. During backpropagation, instead of every GPU accumulating all gradients, you use a reduce-scatter operation. Each GPU only keeps the gradients for its partition of parameters.

83
00:12:53.232 --> 00:12:55.557
<v Eric>So you're saving another chunk of memory.

84
00:12:55.857 --> 00:13:04.947
<v Maya>Right. The gradients were 3 gigabytes in our example. With Stage 2 on 64 GPUs, each GPU only stores about 50 megabytes of gradients.

85
00:13:05.247 --> 00:13:07.154
<v Eric>And then there's Stage 3.

86
00:13:07.454 --> 00:13:15.291
<v Maya>Stage 3 is the full monty. It partitions the parameters themselves. Each GPU only stores 1/Nth of the model parameters at rest.

87
00:13:15.591 --> 00:13:21.155
<v Eric>But wait—you need all the parameters to actually compute the forward and backward passes.

88
00:13:21.455 --> 00:13:35.927
<v Maya>You do, and this is where it gets clever. Just before you need to use a layer's parameters, you do an all-gather to collect them from all GPUs. You compute the forward or backward pass for that layer, and then you can discard the parameters you're not responsible for.

89
00:13:36.227 --> 00:13:38.813
<v Eric>So you're constantly gathering and discarding?

90
00:13:39.113 --> 00:13:52.984
<v Maya>Layer by layer, yes. This means more communication—the paper says about 1.5x the communication of baseline data parallelism. But in return, each GPU now stores only 1/Nth of everything: parameters, gradients, and optimizer states.

91
00:13:53.284 --> 00:13:55.374
<v Eric>With 1000 GPUs...

92
00:13:55.674 --> 00:14:11.217
<v Maya>Each GPU stores 1/1000th of the model states. A model that would need 24 gigabytes on a single GPU now needs 24 megabytes per GPU. You can train models with trillions of parameters—something that was completely impossible before.

93
00:14:12.317 --> 00:14:20.676
<v Eric>I think it would help our listeners to really understand the communication operations here. Can you explain all-reduce, reduce-scatter, and all-gather?

94
00:14:20.976 --> 00:14:25.338
<v Maya>Absolutely. These are the bread and butter of distributed computing, so let's break them down.

95
00:14:25.638 --> 00:14:27.258
<v Eric>Start with all-reduce.

96
00:14:27.558 --> 00:14:44.303
<v Maya>Okay, imagine you have 4 GPUs, each with a value. GPU 0 has 1, GPU 1 has 2, GPU 2 has 3, GPU 3 has 4. An all-reduce sums them up and gives every GPU the result. After all-reduce, every GPU has the value 10.

97
00:14:44.603 --> 00:14:47.842
<v Eric>So everyone contributes, everyone gets the result.

98
00:14:48.142 --> 00:15:04.625
<v Maya>Exactly. Now, reduce-scatter is different. You still sum up the values, but instead of everyone getting the complete result, the result is scattered across GPUs. If we're summing and scattering across 4 pieces, GPU 0 gets the first quarter of the sum, GPU 1 gets the second quarter, and so on.

99
00:15:04.925 --> 00:15:06.858
<v Eric>So the result is distributed.

100
00:15:07.158 --> 00:15:16.771
<v Maya>Right. And all-gather is the opposite of scatter. Each GPU has a piece of data, and after all-gather, every GPU has all the pieces concatenated together.

101
00:15:17.071 --> 00:15:21.199
<v Eric>So reduce-scatter followed by all-gather equals all-reduce.

102
00:15:21.499 --> 00:15:33.567
<v Maya>Exactly! That's the key mathematical identity that makes ZeRO work. You're just reorganizing the communication pattern. Instead of everyone having everything all the time, you distribute storage but gather on-demand when you need it.

103
00:15:33.867 --> 00:15:38.308
<v Eric>And modern communication libraries are really good at implementing these efficiently.

104
00:15:38.608 --> 00:15:52.819
<v Maya>They are. NCCL, which stands for NVIDIA Collective Communications Library, has highly optimized implementations that use ring-based algorithms, tree reductions, and other techniques to minimize latency and maximize bandwidth utilization.

105
00:15:53.119 --> 00:15:58.500
<v Eric>The paper mentions overlapping communication with computation. How does that work?

106
00:15:58.800 --> 00:16:13.873
<v Maya>This is a crucial optimization. Remember that in ZeRO-3, you need to gather parameters before computing each layer. The naive approach would be: gather layer 1 parameters, compute layer 1, gather layer 2 parameters, compute layer 2, and so on.

107
00:16:14.173 --> 00:16:15.975
<v Eric>Serializing everything.

108
00:16:16.275 --> 00:16:29.598
<v Maya>Right, which would be slow. But in practice, while GPU compute is happening for layer 1, the communication hardware can already be fetching layer 2's parameters. Modern GPUs have separate engines for compute and communication that can run concurrently.

109
00:16:29.898 --> 00:16:33.085
<v Eric>So you're pipelining the gathering with the computation.

110
00:16:33.385 --> 00:16:46.002
<v Maya>Exactly. If you structure things right, the communication is almost entirely hidden behind computation. The paper shows that with proper overlap, ZeRO-3's extra communication has minimal impact on end-to-end training time.

111
00:16:47.102 --> 00:16:53.032
<v Eric>Let's get into the quantitative analysis. The paper has some nice formulas for memory consumption.

112
00:16:53.332 --> 00:17:06.001
<v Maya>They do. Let me set up the notation. They use the Greek letter psi, Ψ, to represent the number of model parameters. For mixed precision training with Adam, the memory consumption per GPU in standard data parallelism is:

113
00:17:06.301 --> 00:17:08.756
<v Eric>So that's 16Ψ total bytes.

114
00:17:09.056 --> 00:17:16.763
<v Maya>Right, 16 bytes per parameter. For a 7.5 billion parameter model, that's 120 gigabytes just for model states on every GPU.

115
00:17:17.063 --> 00:17:18.343
<v Eric>And with ZeRO?

116
00:17:18.643 --> 00:17:33.454
<v Maya>With ZeRO Stage 1, you partition the optimizer states. The 12Ψ bytes for fp32 master copy, momentum, and variance get divided by N, the number of GPUs. So your memory becomes 4Ψ plus 12Ψ/N.

117
00:17:33.754 --> 00:17:35.269
<v Eric>And with Stage 2?

118
00:17:35.569 --> 00:17:46.514
<v Maya>The gradients also get partitioned. Now you have 2Ψ for parameters plus 2Ψ/N for gradients plus 12Ψ/N for optimizer states. That simplifies to 2Ψ plus 14Ψ/N.

119
00:17:46.814 --> 00:17:48.094
<v Eric>And Stage 3?

120
00:17:48.394 --> 00:18:00.515
<v Maya>Everything is partitioned. It's simply 16Ψ/N. With 64 GPUs, each GPU stores only 1/64th of those 120 gigabytes. That's less than 2 gigabytes per GPU.

121
00:18:00.815 --> 00:18:04.943
<v Eric>Which means you have tons of memory left for activations.

122
00:18:05.243 --> 00:18:15.927
<v Maya>Exactly. And the paper makes another key observation. As you increase N, the number of GPUs, you can proportionally increase Ψ, the model size, while keeping per-GPU memory constant.

123
00:18:16.227 --> 00:18:17.611
<v Eric>Linear scaling.

124
00:18:17.911 --> 00:18:26.819
<v Maya>Right. Double your GPUs, double your model size. This is fundamentally different from model parallelism where the scaling relationship is more complex.

125
00:18:27.119 --> 00:18:31.246
<v Eric>There's also analysis of when each stage is most beneficial, right?

126
00:18:31.546 --> 00:18:45.104
<v Maya>Yes. ZeRO-1 gives you about 4x memory reduction with no extra communication. ZeRO-2 gives you about 8x with still the same communication as baseline. ZeRO-3 gives you linear reduction with N, but at 1.5x communication.

127
00:18:45.404 --> 00:18:46.971
<v Eric>So there's a tradeoff.

128
00:18:47.271 --> 00:18:57.120
<v Maya>Right. For moderately sized models, ZeRO-1 or ZeRO-2 might be enough. For truly massive models, you need ZeRO-3 and accept the communication overhead.

129
00:18:58.220 --> 00:19:06.344
<v Eric>Let's talk about the practical implementation. The paper introduces ZeRO as part of Microsoft's DeepSpeed library.

130
00:19:06.644 --> 00:19:16.335
<v Maya>Yes, DeepSpeed is their open-source deep learning optimization library. It's built on top of PyTorch and provides a whole suite of optimizations, but ZeRO is the crown jewel.

131
00:19:16.635 --> 00:19:19.091
<v Eric>What does it look like to use in practice?

132
00:19:19.391 --> 00:19:34.019
<v Maya>It's remarkably simple. You have your normal PyTorch model. Instead of wrapping it with DistributedDataParallel, you use DeepSpeed's initialization function. You pass in a config file that specifies which ZeRO stage you want, and the library handles everything else.

133
00:19:34.319 --> 00:19:35.991
<v Eric>Show me a rough example.

134
00:19:36.291 --> 00:19:50.031
<v Maya>Okay, so you'd have something like: model, optimizer, equals deepspeed.initialize, passing in your model, your config, and so on. The config file might have a section that says zero optimization, stage 3. That's literally it.

135
00:19:50.331 --> 00:19:52.447
<v Eric>No changes to the model code?

136
00:19:52.747 --> 00:20:10.432
<v Maya>Almost none. With ZeRO-1 and ZeRO-2, typically zero changes. With ZeRO-3, there's one gotcha: since parameters aren't all present on each GPU, if you want to access parameters outside the forward/backward pass—like for logging or debugging—you need to use a context manager.

137
00:20:10.732 --> 00:20:12.221
<v Eric>What does that look like?

138
00:20:12.521 --> 00:20:23.284
<v Maya>Something like: with deepspeed.zero.GatheredParameters, then you can access model.parameters. The context manager temporarily gathers the full parameters so you can inspect them.

139
00:20:23.584 --> 00:20:30.297
<v Eric>The paper also mentions several optimizations. Gradient bucketing, communication overlap...

140
00:20:30.597 --> 00:20:42.196
<v Maya>Yes. Gradient bucketing is borrowed from standard distributed training. Instead of communicating many small tensors, you batch them into larger buckets. This amortizes the fixed overhead of communication calls.

141
00:20:42.496 --> 00:20:45.500
<v Eric>And there's also CPU offloading mentioned.

142
00:20:45.800 --> 00:21:00.376
<v Maya>That became ZeRO-Offload in a follow-up paper. The idea is that CPU memory is much larger than GPU memory, so you can offload some of the optimizer states to CPU RAM. The tradeoff is PCIe bandwidth, but for large enough models, it's worth it.

143
00:21:01.476 --> 00:21:05.316
<v Eric>Let's talk results. What did the experiments show?

144
00:21:05.616 --> 00:21:16.300
<v Maya>The headline result was training Turing-NLG, a 17 billion parameter model, and showing that they could scale to 100 billion parameters. At the time, this was unprecedented.

145
00:21:16.600 --> 00:21:19.369
<v Eric>How does the performance compare to baselines?

146
00:21:19.669 --> 00:21:26.200
<v Maya>ZeRO-1 gives 4x memory reduction with literally zero performance overhead. Same throughput as standard data parallelism.

147
00:21:26.500 --> 00:21:29.321
<v Eric>Because the communication pattern is equivalent.

148
00:21:29.621 --> 00:21:34.350
<v Maya>Right. ZeRO-2 actually showed slight throughput improvement in some cases, which surprised people.

149
00:21:34.650 --> 00:21:36.322
<v Eric>How is that possible?

150
00:21:36.622 --> 00:21:45.007
<v Maya>Reduced memory pressure means better cache behavior. When you're not pushing GPU memory to its limits, memory access patterns become more efficient.

151
00:21:45.307 --> 00:21:46.561
<v Eric>And ZeRO-3?

152
00:21:46.861 --> 00:22:00.549
<v Maya>ZeRO-3 has about 10-15% throughput reduction due to the extra communication. But the key point is it enables model sizes that are completely impossible otherwise. You're trading a bit of speed for capability you literally didn't have before.

153
00:22:00.849 --> 00:22:03.905
<v Eric>They also compared against model parallelism, right?

154
00:22:04.205 --> 00:22:14.315
<v Maya>Yes, and this was eye-opening. For the same model size, ZeRO-3 matched or exceeded the throughput of pipeline and tensor parallelism, while being much simpler to implement.

155
00:22:14.615 --> 00:22:16.652
<v Eric>And the combination works too.

156
00:22:16.952 --> 00:22:33.514
<v Maya>That's the key insight for training the largest models today. You use tensor parallelism within a node to efficiently utilize NVLink, and ZeRO across nodes for memory efficiency. Models like GPT-3, Megatron-Turing, and most large language models use this hybrid approach.

157
00:22:34.614 --> 00:22:39.055
<v Eric>We're several years out from this paper now. What's the impact been?

158
00:22:39.355 --> 00:22:58.842
<v Maya>Massive. ZeRO is everywhere. DeepSpeed is one of the most widely used training libraries for large models. Hugging Face integrated ZeRO directly into their Trainer API. Pretty much every open-source effort to train large language models—LLaMA, Falcon, you name it—uses ZeRO or ZeRO-inspired techniques.

159
00:22:59.142 --> 00:23:01.546
<v Eric>And there's been a lot of follow-up work.

160
00:23:01.846 --> 00:23:17.963
<v Maya>So much. ZeRO-Offload extended the idea to CPU memory. ZeRO-Infinity pushed it further to NVMe storage—you can train trillion-parameter models on a single machine with enough SSDs. ZeRO++ optimized communication patterns for specific network topologies.

161
00:23:18.263 --> 00:23:20.536
<v Eric>What about PyTorch's FSDP?

162
00:23:20.836 --> 00:23:31.781
<v Maya>Fully Sharded Data Parallel is essentially ZeRO-3 built natively into PyTorch. The PyTorch team collaborated with the DeepSpeed team, and now you can do ZeRO-style training without even using DeepSpeed if you prefer.

163
00:23:32.081 --> 00:23:34.589
<v Eric>Has the idea influenced other frameworks?

164
00:23:34.889 --> 00:23:46.539
<v Maya>Definitely. JAX has similar functionality. Google's work on GSPMD incorporates sharding ideas. The whole field moved toward the insight that data parallelism can be memory-efficient if you partition cleverly.

165
00:23:46.839 --> 00:23:50.314
<v Eric>What made this paper so influential, in your view?

166
00:23:50.614 --> 00:24:11.407
<v Maya>Three things. First, timing. The scaling laws were just being discovered, everyone wanted bigger models, and ZeRO removed the memory barrier. Second, execution. The DeepSpeed library was well-engineered and easy to use. Third, communication. The paper itself is exceptionally clear—it explains the problem, the insight, and the solution in a way that's accessible but rigorous.

167
00:24:11.707 --> 00:24:13.745
<v Eric>Any limitations we should mention?

168
00:24:14.045 --> 00:24:28.987
<v Maya>The main one is that ZeRO addresses model state memory but not activation memory. For very long sequences, activations can still be the bottleneck. You need techniques like activation checkpointing, where you recompute activations during the backward pass instead of storing them.

169
00:24:29.287 --> 00:24:31.220
<v Eric>And network bandwidth matters.

170
00:24:31.520 --> 00:24:40.898
<v Maya>Yes, ZeRO-3 assumes reasonably fast interconnect. On clusters with slow networking, the communication overhead can hurt more than the analysis suggests.

171
00:24:41.998 --> 00:24:45.655
<v Eric>Well, we've covered a lot of ground. Any final thoughts?

172
00:24:45.955 --> 00:25:01.367
<v Maya>Just that this paper is a great example of how a simple, elegant idea can have enormous impact. The insight—eliminate redundancy in data parallelism through partitioning—seems obvious in retrospect. But it took real engineering and theoretical clarity to make it work.

173
00:25:01.667 --> 00:25:14.102
<v Eric>It's also a reminder that systems papers matter. You can have the best algorithms in the world, but if you can't run them at scale, they stay theoretical. ZeRO made the theory practical.

174
00:25:14.402 --> 00:25:25.686
<v Maya>If you're working in ML and haven't read this paper, go read it. It's one of those rare papers that's both foundational and practical. The ideas are elegant, the engineering is solid, and the impact is undeniable.

175
00:25:25.986 --> 00:25:31.080
<v Eric>That's going to do it for today's episode of Strollcast. Thanks for listening, everyone.

176
00:25:31.380 --> 00:25:38.486
<v Maya>If you enjoyed this deep dive, check out our other episodes at strollcast.com. Let us know what papers you want us to cover next.

177
00:25:38.786 --> 00:25:41.372
<v Eric>Until next time, keep strolling.

178
00:25:41.672 --> 00:25:43.788
<v Maya>And may your gradients never explode.

179
00:25:44.088 --> 00:25:46.256
<v Eric>That's still a terrible sign-off.

180
00:25:46.556 --> 00:25:47.993
<v Maya>And I'm still keeping it.
